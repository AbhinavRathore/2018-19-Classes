{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear-regression-demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/CC-MNNIT/2018-19-Classes/blob/master/MachineLearning/2018_08_22_Logical-Rhythm-2/linear_regression.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fM1tzPvICydP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Press shift+enter to execute a cell**"
      ]
    },
    {
      "metadata": {
        "id": "EtGzW6k4-epp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple Linear Regression\n",
        "\n",
        "The goal of this notebook is to demonstrate a linear regression model from the ground up using numpy."
      ]
    },
    {
      "metadata": {
        "id": "k77xaJfA-epq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import HTML\n",
        "from numpy import *\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v8NSLCXfIE13",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Download dataset to colab"
      ]
    },
    {
      "metadata": {
        "id": "-wPsqGPFIUn4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ceb1626a-8a5a-4fa1-fa6c-715162e2c271"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data.csv  datalab  sample_data\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIdjicszDkAf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/CC-MNNIT/2018-19-Classes/master/MachineLearning/2018_08_22_Logical-Rhythm-2/data.csv\", \"data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FcjydmnBEn2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZU4LfMDe-ept",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import the data\n",
        "Here, we're using a dataset with two columns containing the amount of hours studied and the test scores students achieved, respectively."
      ]
    },
    {
      "metadata": {
        "id": "E9HoBdK3-epu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = genfromtxt('https://github.com/packetChor/ScorePredictor/blob/master/classifier.csv', delimiter=',')\n",
        "\n",
        "#Extract columns\n",
        "x = array(data[:,0])\n",
        "y = array(data[:,1])\n",
        "\n",
        "#Plot the dataset\n",
        "plt.scatter(x,y)\n",
        "plt.xlabel('Hours of study')\n",
        "plt.ylabel('Test scores')\n",
        "plt.title('Dataset')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9jBqpSy-epx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Defining the hyperparamters"
      ]
    },
    {
      "metadata": {
        "id": "VgOidX2L-epy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyperparamters\n",
        "learning_rate = 0.0001\n",
        "initial_b = 0\n",
        "initial_m = 0\n",
        "num_iterations = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2FOk0Nc-ep1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define cost function"
      ]
    },
    {
      "metadata": {
        "id": "kiXWph9g-ep1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(b, m, data):\n",
        "    total_cost = 0\n",
        "    \n",
        "    # number of datapoints in training data\n",
        "    N = float(len(data))\n",
        "    \n",
        "    # Compute sum of squared errors\n",
        "    for i in range(0, len(data)):\n",
        "        x = data[i, 0]\n",
        "        y = data[i, 1]\n",
        "        total_cost += (y - (m * x + b)) ** 2\n",
        "        \n",
        "    # Return average of squared error\n",
        "    return total_cost/(2*N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSObpG-b-ep4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define Gradient Descent functions"
      ]
    },
    {
      "metadata": {
        "id": "z7dQ8n5m-ep5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def step_gradient(b_current, m_current, data, alpha):\n",
        "    \"\"\"takes one step down towards the minima\n",
        "    \n",
        "    Args:\n",
        "        b_current (float): current value of b\n",
        "        m_current (float): current value of m\n",
        "        data (np.array): array containing the training data (x,y)\n",
        "        alpha (float): learning rate / step size\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (b,m) new values of b,m\n",
        "    \"\"\"\n",
        "    \n",
        "    m_gradient = 0\n",
        "    b_gradient = 0\n",
        "    N = float(len(data))\n",
        "\n",
        "    # Calculate Gradient\n",
        "    for i in range(0, len(data)):\n",
        "        x = data[i, 0]\n",
        "        y = data[i, 1]\n",
        "        m_gradient += - (2/N) * x * (y - (m_current * x + b_current))\n",
        "        b_gradient += - (2/N) * (y - (m_current * x + b_current))\n",
        "    \n",
        "    # Update current m and b\n",
        "    m_updated = m_current - alpha * m_gradient\n",
        "    b_updated = b_current - alpha * b_gradient\n",
        "\n",
        "    #Return updated parameters\n",
        "    return b_updated, m_updated\n",
        "\n",
        "def gradient_descent(data, starting_b, starting_m, learning_rate, num_iterations):\n",
        "    \"\"\"runs gradient descent\n",
        "    \n",
        "    Args:\n",
        "        data (np.array): training data, containing x,y\n",
        "        starting_b (float): initial value of b (random)\n",
        "        starting_m (float): initial value of m (random)\n",
        "        learning_rate (float): hyperparameter to adjust the step size during descent\n",
        "        num_iterations (int): hyperparameter, decides the number of iterations for which gradient descent would run\n",
        "    \n",
        "    Returns:\n",
        "        list : the first and second item are b, m respectively at which the best fit curve is obtained, the third and fourth items are two lists, which store the value of b,m as gradient descent proceeded.\n",
        "    \"\"\"\n",
        "\n",
        "    # initial values\n",
        "    b = starting_b\n",
        "    m = starting_m\n",
        "    \n",
        "    # to store the cost after each iteration\n",
        "    cost_graph = []\n",
        "    \n",
        "    # to store the value of b -> bias unit, m-> slope of line after each iteration (pred = m*x + b)\n",
        "    b_progress = []\n",
        "    m_progress = []\n",
        "    \n",
        "    # For every iteration, optimize b, m and compute its cost\n",
        "    for i in range(num_iterations):\n",
        "        cost_graph.append(compute_cost(b, m, data))\n",
        "        b, m = step_gradient(b, m, array(data), learning_rate)\n",
        "        b_progress.append(b)\n",
        "        m_progress.append(m)\n",
        "        \n",
        "    return [b, m, cost_graph,b_progress,m_progress]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5v-y-qI3-ep7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Run gradient_descent() to get optimized parameters b and m"
      ]
    },
    {
      "metadata": {
        "id": "WV6KIdPq-ep7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "b, m, cost_graph,b_progress,m_progress = gradient_descent(data, initial_b, initial_m, learning_rate, num_iterations)\n",
        "\n",
        "#Print optimized parameters\n",
        "print ('Optimized b:', b)\n",
        "print ('Optimized m:', m)\n",
        "\n",
        "#Print error with optimized parameters\n",
        "print ('Minimized cost:', compute_cost(b, m, data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wnmqepye-ep_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Plotting the cost per iterations"
      ]
    },
    {
      "metadata": {
        "id": "x1VREIE7-eqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cost_graph)\n",
        "plt.xlabel('No. of iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost per iteration')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWQAkFE8-eqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gradient descent converges to local minimum after 5 iterations"
      ]
    },
    {
      "metadata": {
        "id": "OT2wKbHu-eqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Plot line of best fit"
      ]
    },
    {
      "metadata": {
        "id": "hD531y8K-eqF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plot dataset\n",
        "plt.scatter(x, y)\n",
        "#Predict y values\n",
        "pred = m * x + b\n",
        "#Plot predictions as line of best fit\n",
        "plt.plot(x, pred, c='r')\n",
        "plt.xlabel('Hours of study')\n",
        "plt.ylabel('Test scores')\n",
        "plt.title('Line of best fit')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XeXdBWc-Cic-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient descent's progress with num of iterations"
      ]
    },
    {
      "metadata": {
        "id": "pHGbi8nY-eqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "b = b_progress[0]\n",
        "m = m_progress[0]\n",
        "pred = m*x + b\n",
        "\n",
        "line = ax.plot(x,pred, '-',c='r')[0]\n",
        "\n",
        "def animate(i,b_prog,m_prog):\n",
        "    pred = m_prog[i] * x + b_prog[i]\n",
        "    line.set_data(x,pred)\n",
        "    return line,\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(b_progress), fargs=(b_progress,m_progress,))\n",
        "ax.scatter(x,y)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}